---
title: "Psychometric Properties of PAES Math M1"
output:
  word_document:
    reference_docx: Template_APA7th.docx
bibliography: references.bib
link-citations: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = F, message = F, eval = F)
```

# Introduction

With respect to its psychometric properties, what are the characteristics of the most appropriate IRT model for the PAES math M1 assessment? How comparable are information and efficiency across forms based on the most appropriate IRT model?

# Method

## Dataset

The data correspond to the student answers in the Mathematics Competence 1 (referred to as **M1** throughout) selection test applied in the 2024 regular admission process. All the information was collected, processed, and made public by the Department of Educational Evaluation, Measurement, and Registration (DEMRE by its acronym in Spanish) of the University of Chile, the institution in charge of the admission process.

The responses were organized in wide format according to the student ID. Importantly, double marks and omitted questions were treated as missing data, given that they are not synonym of wrong answers and we just analyzed correct and incorrect responses.

## Instrument

The M1 test measures abilities (e.g., modeling and representing data) and knowledge (e.g., in numbers, algebra, and probability) in mathematics, which are considered necessary for students to solve diverse problems in day-to-day life. These abilities and knowledge are part of the national curriculum in mathematics and are supposed to be developed during primary and secondary education.

The 2024 M1 test was composed of 65 multiple-choice questions, in which 60 were used to calculate scores and 5 were pilot questions. Each question had 4 options and a single answer. The test had 4 equivalent forms (originally labeled as 113, 114, 115, and 116) that for convenience were referred to as forms **A**, **B**, **C**, and **D** throughout. Forms were prepared in pairs: A and C, and B and D. Each pair of forms had the same questions randomly ordered. Across forms, 47 out of the 60 (78,5%) scored questions were the same.

## Sample

The original dataset contained the responses of 243,305 students who took the M1 test in 2024. After revising the data, cases with no responses were removed, thus the sample slightly decreased to 243,294 students. Because of the focus of this study, information about students is not provided; However, a detailed description of this can be found on the DEMRE website.

## Procedure
<<<<<<< HEAD
### IRT Models
In order to select the most appropriate item response theory (IRT) model , we investigated the appropriateness of the 1PL, 2PL, and 3PL IRT models. Both the absolute and relative fits of the models were evaluated, leading to the selection of the most appropriate model. The 1PL, 2PL, and 3PL IRT models belong to a family of psychometric models suitable for assessing a unidimensional latent trait when items are dichotomously scored (i.e., correct or incorrect), as is the case with the PAES assessment.

The three models differ based on the number of parameters being estimated. In the 1PL model, only the item difficulty parameter (b) is estimated for each item, while the discrimination parameter (a) is constant across items. The 2PL model includes an additional item discrimination parameter (a) estimated for each item. The 3PL model further includes a guessing parameter (c) estimated for each item. The mathematical form of the 3PL model is:

$$
P(X_i = 1|\theta) = c_i + (1 - c_i) \frac{exp{(a_i(\theta - b_i))}}{1 + exp{(a_i(\theta - b_i))}}
$$

where:
- \( P(X_i = 1|\theta) \) is the probability of a correct response by a person with ability level \( \theta \)
- \( a_i \) is the discrimination parameter for item \( i \)
- \( b_i \) is the difficulty parameter for item \( i \)
- \( c_i \) is the guessing parameter for item \( i \)

The 2PL model can be derived from the equation above by setting the guessing parameter (\( c_i \)) to zero, and the 1PL can be derived from the 2PL by setting the discrimination parameter (\( a_i \)) to a constant.

The discrimination (\( a_i \)) parameter indicates how effectively an item distinguishes between individuals with varying levels of ability. Items with high discrimination provide valuable insights into differences in ability level across individuals.

The difficulty parameter (\( b_i \)) denotes the point on the ability scale where the probability of a correct response equals 0.5 for the 1PL and 2PL models. For the 3PL model, it denotes the point where the probability of a correct response is halfway between the guessing parameter (\( c_i \)) and 1. (i.e $$ P(X_i = 1|\theta) = \frac{1+c_i}{2} $$).

The guessing parameter (\( b_i \)) represents the probability of providing a correct response regardless of an individual's ability. 
=======

This study investigated the appropriateness of the 1PL, 2PL and 3PL IRT models.
>>>>>>> ce7fac6ef1ff92900f2c90894236eb435432c3a9

IRT models (parameters)
Calibration (estimation methods and concurrent calibration - equating)
Absolute model fit assessment
Model comparisons based on test information and reliability. Relative model fit assessment
Forms comparison based on the best-fitting model (comparing test info and relative efficiency)

# Results

Comparison of model results (information, factor scores, SE)
Comparison of forms based on Rasch and 2PL models (information, expected true scores)

# Discussion

# Conclusion
