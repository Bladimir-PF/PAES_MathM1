---
title: "Psychometric Properties of PAES Math M1"
output:
  word_document:
    reference_docx: Template_APA7th.docx
bibliography: references.bib
link-citations: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = F, message = F, eval = F)
```

# Introduction

With respect to its psychometric properties, what are the characteristics of the most appropriate IRT model for the PAES math M1 assessment? How comparable are information and efficiency across forms based on the most appropriate IRT model?

# Method

## Dataset

The data correspond to the student answers in the Mathematics Competence 1 (referred to as **M1** throughout) selection test applied in the 2024 regular admission process. All the information was collected, processed, and made public by the Department of Educational Evaluation, Measurement, and Registration (DEMRE by its acronym in Spanish) of the University of Chile, the institution in charge of the admission process.

The responses were organized in wide format according to the student ID. Importantly, double marks and omitted questions were treated as missing data, given that they are not synonym of wrong answers and we just analyzed correct and incorrect responses.

## Instrument

The M1 test measures abilities (e.g., modeling and representing data) and knowledge (e.g., in numbers, algebra, and probability) in mathematics, which are considered necessary for students to solve diverse problems in day-to-day life. These abilities and knowledge are part of the national curriculum in mathematics and are supposed to be developed during primary and secondary education.

The 2024 M1 test was composed of 65 multiple-choice questions, in which 60 were used to calculate scores and 5 were pilot questions. Each question had 4 options and a single answer. The test had 4 equivalent forms (originally labeled as 113, 114, 115, and 116) that for convenience were referred to as forms **A**, **B**, **C**, and **D** throughout. Forms were prepared in pairs: A and C, and B and D. Each pair of forms had the same questions randomly ordered. Across forms, 47 out of the 60 (78,5%) scored questions were the same.

## Sample

The original dataset contained the responses of 243,305 students who took the M1 test in 2024. After revising the data, cases with no responses were removed, thus the sample slightly decreased to 243,294 students. Because of the focus of this study, information about students is not provided; However, a detailed description of this can be found on the DEMRE website.

## Procedure

This study investigated the appropriateness of the 1PL, 2PL and 3PL IRT models.

IRT models (parameters)
Calibration (estimation methods and concurrent calibration - equating)
Absolute model fit assessment
Model comparisons based on test information and reliability. Relative model fit assessment
Forms comparison based on the best-fitting model (comparing test info and relative efficiency)

# Results

Comparison of model results (information, factor scores, SE)
Comparison of forms based on Rasch and 2PL models (information, expected true scores)

# Discussion

# Conclusion
